{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7342687a-4929-4843-983c-f0da7726bd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import gym_maze\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0152c9d-83c5-4d80-b721-42f1e8e6bf91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.3787600000000002\n",
      "0.35527999999999976\n",
      "0.8494799999999999\n",
      "0.8908\n",
      "0.9286\n",
      "0.93436\n",
      "0.9336800000000003\n",
      "0.93612\n",
      "0.92168\n",
      "0.9329999999999998\n",
      "0.9360399999999999\n",
      "0.9249199999999999\n",
      "0.9349200000000001\n",
      "0.9352399999999998\n",
      "0.93884\n",
      "0.90748\n",
      "0.9400799999999998\n",
      "0.9331599999999999\n",
      "0.93964\n",
      "0.9348800000000002\n",
      "0.9358\n",
      "0.9359199999999998\n",
      "0.9368799999999999\n",
      "0.9322\n",
      "0.9337199999999999\n",
      "0.9334\n",
      "0.9221199999999999\n",
      "0.9258799999999999\n",
      "0.9216\n",
      "0.9204399999999999\n",
      "0.9383599999999997\n",
      "0.9406399999999999\n",
      "0.93372\n",
      "0.9252400000000001\n",
      "0.9375199999999999\n",
      "0.93848\n",
      "0.9364\n",
      "0.9133999999999999\n",
      "0.9191599999999999\n",
      "0.93852\n",
      "0.9350799999999998\n",
      "0.9353599999999999\n",
      "0.9352799999999999\n",
      "0.9321200000000001\n",
      "0.9355999999999999\n",
      "0.9128399999999998\n",
      "0.93152\n",
      "0.9348399999999999\n",
      "0.93472\n",
      "0.9373600000000001\n",
      "0.9214000000000001\n",
      "0.93128\n",
      "0.9363199999999999\n",
      "0.93792\n",
      "0.9251999999999997\n",
      "0.9351199999999998\n",
      "0.9395199999999998\n",
      "0.9219199999999997\n",
      "0.9257599999999999\n",
      "0.92964\n",
      "0.9362\n",
      "0.9393600000000001\n",
      "0.93448\n",
      "0.9388799999999999\n",
      "0.9303199999999998\n",
      "0.9250399999999999\n",
      "0.9358399999999999\n",
      "0.93924\n",
      "0.92216\n",
      "0.9195199999999999\n",
      "0.93488\n",
      "0.93588\n",
      "0.93928\n",
      "0.93816\n",
      "0.93684\n",
      "0.93616\n",
      "0.9356399999999999\n",
      "0.9358399999999999\n",
      "0.9403199999999998\n",
      "0.9321999999999997\n",
      "0.9234799999999997\n",
      "0.90848\n",
      "0.9373200000000002\n",
      "0.9343999999999998\n",
      "0.9364399999999997\n",
      "0.93428\n",
      "0.9363599999999999\n",
      "0.9356399999999998\n",
      "0.93204\n",
      "0.9364799999999999\n",
      "0.93536\n",
      "0.9347999999999999\n",
      "0.9358399999999998\n",
      "0.9377199999999998\n",
      "0.9343999999999998\n",
      "0.9099599999999998\n",
      "0.9355199999999999\n",
      "0.9340399999999999\n",
      "0.9387599999999999\n",
      "0.9227199999999999\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('maze-sample-5x5-v0')\n",
    "state_n = 25\n",
    "action_n = 4\n",
    "N = 100\n",
    "\n",
    "class RandomAgent():\n",
    "    def __init__(self, action_n):\n",
    "        self.action_n = action_n\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        return random.randint(0, self.action_n-1)\n",
    "\n",
    "    \n",
    "class CEM():\n",
    "    def __init__(self, state_n, action_n):\n",
    "        self.state_n = state_n\n",
    "        self.action_n = action_n\n",
    "        self.policy = np.ones((self.state_n, self.action_n)) / self.action_n\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        return int(np.random.choice(np.arange(action_n),\n",
    "            p = self.policy[state, :]))\n",
    "    def update_policy(self, elite_trajectories):\n",
    "        \n",
    "        pre_policy = np.ones((self.state_n, self.action_n)) / self.action_n\n",
    "        \n",
    "        for trajectory in elite_trajectories:\n",
    "            for state, action in zip(trajectory['states'], trajectory['actions']):\n",
    "                pre_policy[state][action] += 1\n",
    "        for state in range(self.state_n):\n",
    "            if sum(pre_policy[state])>0:\n",
    "                self.policy[state] = pre_policy[state] / sum(pre_policy[state])\n",
    "                \n",
    "    \n",
    "def get_state(obs):\n",
    "    return int(obs[1] * np.sqrt(state_n) + obs[0])\n",
    "\n",
    "def get_trajectory(agent, trajectory_len):\n",
    "    \n",
    "    trajectory = {'states':[],\n",
    "                 'actions':[],\n",
    "                 'total_reward': 0}\n",
    "    \n",
    "    obs = env.reset()\n",
    "    state = get_state(obs)\n",
    "    \n",
    "    for _ in range(trajectory_len):\n",
    "        \n",
    "        action = agent.get_action(state)\n",
    "        trajectory['states'].append(state)\n",
    "        trajectory['actions'].append(action)\n",
    "        \n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        state = get_state(obs)\n",
    "        \n",
    "        trajectory['total_reward']+= reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    return trajectory\n",
    "\n",
    "\n",
    "def get_elite_trajectories(trajectories, q_param):\n",
    "    \n",
    "    quantile = np.quantile([trajectory['total_reward'] for trajectory in trajectories],\n",
    "                q_param)\n",
    "    \n",
    "    elite_trajectories = [trajectory for trajectory in trajectories if trajectory['total_reward']> quantile]\n",
    "    return elite_trajectories\n",
    "    \n",
    "    \n",
    "\n",
    "agent = CEM(state_n, action_n)\n",
    "\n",
    "epoch_n = 100\n",
    "trajectory_n = 100 # k\n",
    "trajectory_len = 100\n",
    "q_param = 0.9\n",
    "\n",
    "for _ in range(epoch_n):\n",
    "    trajectories = [get_trajectory(agent, trajectory_len) for _ in range(trajectory_n)]\n",
    "    \n",
    "    mean_total_reward = np.mean([trajectory['total_reward'] for trajectory in trajectories])\n",
    "    print(mean_total_reward)\n",
    "    elite_trajectories = get_elite_trajectories(trajectories, q_param)\n",
    "    \n",
    "    if len(elite_trajectories)>0:\n",
    "        agent.update_policy(elite_trajectories)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8c98f87-2ea5-4d6c-b9d9-1b183feac9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.]\n",
      "[1 0]\n",
      "[0 0]\n",
      "[1 0]\n",
      "[2 0]\n",
      "[2 1]\n",
      "[2 2]\n",
      "[2 3]\n",
      "[2 4]\n",
      "[3 4]\n",
      "[3 3]\n",
      "[3 2]\n",
      "[3 2]\n",
      "[3 1]\n",
      "[4 1]\n",
      "[4 2]\n",
      "[4 3]\n",
      "[4 4]\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "state = get_state(obs)\n",
    "print(obs)\n",
    "for _ in range(trajectory_len):\n",
    "    action = agent.get_action(state)\n",
    "    \n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    \n",
    "    print(obs)\n",
    "    \n",
    "    state = get_state(obs)\n",
    "    \n",
    "    if done:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ods_rl",
   "language": "python",
   "name": "ods_rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
