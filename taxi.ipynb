{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "502a9b04-c051-4983-aeb0-6f9747b811d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "env = gym.make('Taxi-v3', new_step_api=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf7838cc-863b-4b03-80db-75af899f8244",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 72\u001b[0m\n\u001b[0;32m     69\u001b[0m q_param \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.8\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch_n):\n\u001b[1;32m---> 72\u001b[0m     trajectories \u001b[38;5;241m=\u001b[39m [get_trajectory(agent, trajectory_len) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(trajectory_n)]\n\u001b[0;32m     74\u001b[0m     mean_total_reward \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean([trajectory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_reward\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m trajectory \u001b[38;5;129;01min\u001b[39;00m trajectories])\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;66;03m# print(mean_total_reward)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[25], line 72\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     69\u001b[0m q_param \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.8\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch_n):\n\u001b[1;32m---> 72\u001b[0m     trajectories \u001b[38;5;241m=\u001b[39m [\u001b[43mget_trajectory\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrajectory_len\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(trajectory_n)]\n\u001b[0;32m     74\u001b[0m     mean_total_reward \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean([trajectory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_reward\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m trajectory \u001b[38;5;129;01min\u001b[39;00m trajectories])\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;66;03m# print(mean_total_reward)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[25], line 39\u001b[0m, in \u001b[0;36mget_trajectory\u001b[1;34m(agent, trajectory_len)\u001b[0m\n\u001b[0;32m     35\u001b[0m state \u001b[38;5;241m=\u001b[39m get_state(obs)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(trajectory_len):\n\u001b[1;32m---> 39\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m     trajectory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstates\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(state)\n\u001b[0;32m     41\u001b[0m     trajectory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactions\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(action)\n",
      "Cell \u001b[1;32mIn[25], line 11\u001b[0m, in \u001b[0;36mCEM.get_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_n\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "state_n = 500\n",
    "action_n = 6\n",
    "\n",
    "class CEM():\n",
    "    def __init__(self, state_n, action_n):\n",
    "        self.state_n = state_n\n",
    "        self.action_n = action_n\n",
    "        self.policy = np.ones((self.state_n, self.action_n)) / self.action_n\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        return int(np.random.choice(np.arange(action_n),\n",
    "            p = self.policy[state, :]))\n",
    "    def update_policy(self, elite_trajectories):\n",
    "        \n",
    "        pre_policy = np.ones((self.state_n, self.action_n)) / self.action_n\n",
    "        \n",
    "        for trajectory in elite_trajectories:\n",
    "            for state, action in zip(trajectory['states'], trajectory['actions']):\n",
    "                pre_policy[state][action] += 1\n",
    "        for state in range(self.state_n):\n",
    "            if sum(pre_policy[state])>0:\n",
    "                self.policy[state] = pre_policy[state] / sum(pre_policy[state])\n",
    "                \n",
    "def get_state(obs):\n",
    "    return int(obs)\n",
    "\n",
    "\n",
    "def get_trajectory(agent, trajectory_len):\n",
    "    \n",
    "    trajectory = {'states':[],\n",
    "                 'actions':[],\n",
    "                 'total_reward': 0}\n",
    "    \n",
    "    obs = env.reset()\n",
    "    state = get_state(obs)\n",
    "    \n",
    "    for _ in range(trajectory_len):\n",
    "        \n",
    "        action = agent.get_action(state)\n",
    "        trajectory['states'].append(state)\n",
    "        trajectory['actions'].append(action)\n",
    "        \n",
    "        obs, reward, done, _, _  = env.step(action)\n",
    "        state = get_state(obs)\n",
    "        \n",
    "        trajectory['total_reward']+= reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            print('done')\n",
    "    return trajectory\n",
    "\n",
    "\n",
    "def get_elite_trajectories(trajectories, q_param):\n",
    "    \n",
    "    quantile = np.quantile([trajectory['total_reward'] for trajectory in trajectories],\n",
    "                q_param)\n",
    "    \n",
    "    elite_trajectories = [trajectory for trajectory in trajectories if trajectory['total_reward']> quantile]\n",
    "    return elite_trajectories\n",
    "    \n",
    "    \n",
    "\n",
    "agent = CEM(state_n, action_n)\n",
    "\n",
    "epoch_n = 1000\n",
    "trajectory_n = 100 # k\n",
    "trajectory_len = 1000\n",
    "q_param = 0.8\n",
    "\n",
    "for _ in range(epoch_n):\n",
    "    trajectories = [get_trajectory(agent, trajectory_len) for _ in range(trajectory_n)]\n",
    "    \n",
    "    mean_total_reward = np.mean([trajectory['total_reward'] for trajectory in trajectories])\n",
    "    # print(mean_total_reward)\n",
    "    elite_trajectories = get_elite_trajectories(trajectories, q_param)\n",
    "    \n",
    "    if len(elite_trajectories)>0:\n",
    "        agent.update_policy(elite_trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43a6e421-9757-4f7a-9b6e-9763a00e24a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,\n",
       "        0.16666667],\n",
       "       [0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,\n",
       "        0.16666667],\n",
       "       [0.03240741, 0.22685185, 0.25462963, 0.28240741, 0.19907407,\n",
       "        0.00462963],\n",
       "       ...,\n",
       "       [0.05555556, 0.38888889, 0.38888889, 0.05555556, 0.05555556,\n",
       "        0.05555556],\n",
       "       [0.05555556, 0.38888889, 0.38888889, 0.05555556, 0.05555556,\n",
       "        0.05555556],\n",
       "       [0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,\n",
       "        0.16666667]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ods_rl",
   "language": "python",
   "name": "ods_rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
